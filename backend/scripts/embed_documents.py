"""
ë¬¸ì„œ ì„ë² ë”© ìŠ¤í¬ë¦½íŠ¸
PDF ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ì—¬ Vector Storeì— ì €ì¥í•©ë‹ˆë‹¤.
ì¤‘ë³µ ì„ë² ë”© ë°©ì§€: ì´ë¯¸ ì„ë² ë”©ëœ ë¬¸ì„œëŠ” ìŠ¤í‚µí•©ë‹ˆë‹¤.
"""
import os
import sys
import hashlib
from pathlib import Path
from typing import List, Dict, Set
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.core.config import settings
from app.utils.openai_client import get_openai_client

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë¬¸ì„œ ë””ë ‰í† ë¦¬
DOCUMENTS_DIR = project_root / "data" / "documents"
VECTOR_STORE_PATH = project_root / "data" / "vector_store"

# Chroma ì‚¬ìš© (ë¡œì»¬ Vector Store)
try:
    import chromadb
    from chromadb.config import Settings
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False
    logger.warning("chromadbê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install chromadb' ì‹¤í–‰ í•„ìš”")


def extract_text_from_pdf(pdf_path: Path) -> str:
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        import PyPDF2
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text
    except ImportError:
        logger.error("PyPDF2ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install PyPDF2' ì‹¤í–‰ í•„ìš”")
        raise
    except Exception as e:
        logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {pdf_path}, error: {str(e)}")
        raise


def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk.strip())
        start = end - overlap  # ì˜¤ë²„ë©ìœ¼ë¡œ ë¬¸ë§¥ ìœ ì§€
    return chunks


def get_file_hash(file_path: Path) -> str:
    """íŒŒì¼ì˜ í•´ì‹œê°’ ê³„ì‚° (ìˆ˜ì • ì‹œê°„ + í¬ê¸° ê¸°ë°˜)"""
    stat = file_path.stat()
    # íŒŒì¼ í¬ê¸° + ìˆ˜ì • ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì‹œ ìƒì„±
    content = f"{stat.st_size}_{stat.st_mtime}"
    return hashlib.md5(content.encode()).hexdigest()


def get_existing_ids(collection, source_name: str, file_stem: str) -> Set[str]:
    """ê¸°ì¡´ì— ì„ë² ë”©ëœ ë¬¸ì„œ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # í•´ë‹¹ ì†ŒìŠ¤ì˜ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
        results = collection.get(
            where={"source": source_name},
            include=["metadatas"]
        )
        
        # ê°™ì€ íŒŒì¼ì˜ IDë§Œ í•„í„°ë§
        existing_ids = set()
        if results["ids"] and results["metadatas"]:
            for idx, metadata in enumerate(results["metadatas"]):
                if metadata and metadata.get("file", "").startswith(file_stem):
                    existing_ids.add(results["ids"][idx])
        
        return existing_ids
    except Exception as e:
        logger.warning(f"ê¸°ì¡´ ID ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return set()


def embed_documents():
    """ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ì—¬ Vector Storeì— ì €ì¥"""
    if not CHROMA_AVAILABLE:
        logger.error("chromadbë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install chromadb")
        return
    
    if not settings.OPENAI_API_KEY:
        logger.error("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # Chroma í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    VECTOR_STORE_PATH.mkdir(parents=True, exist_ok=True)
    client = chromadb.PersistentClient(path=str(VECTOR_STORE_PATH))
    
    # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
    collection = client.get_or_create_collection(
        name="pet_food_rag",
        metadata={"description": "Pet Food RAG Documents"}
    )
    
    existing_count = collection.count()
    logger.info(f"ğŸ“Š ê¸°ì¡´ ë¬¸ì„œ ìˆ˜: {existing_count}ê°œ")
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸
    openai_client = get_openai_client()
    
    # ë¬¸ì„œ ë””ë ‰í† ë¦¬ ìˆœíšŒ
    total_chunks = 0
    skipped_chunks = 0
    new_chunks = 0
    
    for doc_folder in DOCUMENTS_DIR.iterdir():
        if not doc_folder.is_dir():
            continue
        
        source_name = doc_folder.name
        logger.info(f"ğŸ“ Processing {source_name}...")
        
        # PDF íŒŒì¼ ì°¾ê¸°
        pdf_files = list(doc_folder.glob("*.pdf"))
        if not pdf_files:
            logger.info(f"  âš ï¸  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {source_name}")
            continue
        
        for pdf_file in pdf_files:
            logger.info(f"  ğŸ“„ Processing {pdf_file.name}...")
            
            try:
                # íŒŒì¼ í•´ì‹œ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)
                file_hash = get_file_hash(pdf_file)
                file_stem = pdf_file.stem
                
                # ê¸°ì¡´ì— ì„ë² ë”©ëœ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
                existing_ids = get_existing_ids(collection, source_name, file_stem)
                
                if existing_ids:
                    logger.info(f"  â„¹ï¸  ê¸°ì¡´ ì„ë² ë”© ë°œê²¬: {len(existing_ids)}ê°œ ì²­í¬")
                    # TODO: íŒŒì¼ í•´ì‹œ ë¹„êµë¡œ ë³€ê²½ ê°ì§€ (í˜„ì¬ëŠ” ID ì¡´ì¬ ì—¬ë¶€ë§Œ ì²´í¬)
                
                # PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = extract_text_from_pdf(pdf_file)
                if not text.strip():
                    logger.warning(f"  âš ï¸  í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {pdf_file.name}")
                    continue
                
                logger.info(f"  âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text)}ì")
                
                # ì²­í¬ë¡œ ë¶„í• 
                chunks = chunk_text(text, chunk_size=500, overlap=50)
                logger.info(f"  ğŸ“¦ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
                
                # ê° ì²­í¬ ì„ë² ë”© (ì‹ ê·œë§Œ)
                embeddings = []
                documents = []
                metadatas = []
                ids = []
                
                for idx, chunk in enumerate(chunks):
                    chunk_id = f"{source_name}_{file_stem}_{idx}"
                    
                    # ì´ë¯¸ ì„ë² ë”©ëœ ì²­í¬ëŠ” ìŠ¤í‚µ
                    if chunk_id in existing_ids:
                        skipped_chunks += 1
                        if (idx + 1) % 50 == 0:
                            logger.info(f"    ìŠ¤í‚µ ì¤‘: {idx + 1}/{len(chunks)} (ì´ë¯¸ ì„ë² ë”©ë¨)")
                        continue
                    
                    try:
                        # OpenAIë¡œ ì„ë² ë”© ìƒì„±
                        response = openai_client.embeddings.create(
                            model="text-embedding-3-small",  # ì €ë ´í•˜ê³  ë¹ ë¥¸ ëª¨ë¸
                            input=chunk
                        )
                        embedding = response.data[0].embedding
                        
                        embeddings.append(embedding)
                        documents.append(chunk)
                        metadatas.append({
                            "source": source_name,
                            "file": pdf_file.name,
                            "file_hash": file_hash,  # íŒŒì¼ í•´ì‹œ ì €ì¥ (ë³€ê²½ ê°ì§€ìš©)
                            "chunk_index": idx,
                            "total_chunks": len(chunks)
                        })
                        ids.append(chunk_id)
                        new_chunks += 1
                        
                        if (idx + 1) % 10 == 0:
                            logger.info(f"    ì§„í–‰ ì¤‘: {idx + 1}/{len(chunks)} (ì‹ ê·œ: {new_chunks}, ìŠ¤í‚µ: {skipped_chunks})")
                    
                    except Exception as e:
                        logger.error(f"    âŒ ì²­í¬ {idx} ì„ë² ë”© ì‹¤íŒ¨: {str(e)}")
                        continue
                
                # Chromaì— ì¼ê´„ ì €ì¥ (ì‹ ê·œ ì²­í¬ë§Œ)
                if embeddings:
                    try:
                        collection.add(
                            embeddings=embeddings,
                            documents=documents,
                            metadatas=metadatas,
                            ids=ids
                        )
                        total_chunks += len(embeddings)
                        logger.info(f"  âœ… {len(embeddings)}ê°œ ì‹ ê·œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
                    except Exception as e:
                        # ì¤‘ë³µ ID ì—ëŸ¬ ì²˜ë¦¬
                        if "duplicate" in str(e).lower() or "already exists" in str(e).lower():
                            logger.warning(f"  âš ï¸  ì¼ë¶€ ì²­í¬ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {str(e)}")
                        else:
                            logger.error(f"  âŒ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                else:
                    logger.info(f"  â„¹ï¸  ëª¨ë“  ì²­í¬ê°€ ì´ë¯¸ ì„ë² ë”©ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            
            except Exception as e:
                logger.error(f"  âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {pdf_file.name}, error: {str(e)}")
                continue
    
    final_count = collection.count()
    logger.info("=" * 60)
    logger.info(f"âœ… ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ì‹ ê·œ ì €ì¥: {total_chunks}ê°œ ì²­í¬")
    logger.info(f"â­ï¸  ìŠ¤í‚µë¨: {skipped_chunks}ê°œ ì²­í¬ (ì´ë¯¸ ì„ë² ë”©ë¨)")
    logger.info(f"ğŸ“ˆ ìµœì¢… ë¬¸ì„œ ìˆ˜: {final_count}ê°œ (ê¸°ì¡´: {existing_count}ê°œ)")
    logger.info("=" * 60)


if __name__ == "__main__":
    embed_documents()
